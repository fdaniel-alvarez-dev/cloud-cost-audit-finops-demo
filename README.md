# Cloud Cost Audit for SaaS | 20% Savings on $20k Monthly Spend
**Role:** FinOps / Cloud Cost Optimization Consultant (AWS/GCP)

SaaS company with rising cloud spend (~$20k/month) needed fast savings without risking performance. I performed a focused AWS/GCP cost audit, identified waste drivers, prioritized 10 quick wins, and delivered a cost-visibility dashboard plus an executive-friendly report. Outcome: ~20% savings and a practical monthly optimization plan to prevent cost drift.

## Business Challenge
Cloud costs were increasing faster than revenue, and the team needed savings quickly without introducing reliability risk or slowing product delivery.

## Strategic Approach
I ran a targeted, two-provider FinOps audit focused on:
- Cost visibility and allocation readiness (tags/labels, unallocated spend)
- Waste driver detection (underutilized compute, zombie assets, storage tiering, egress hotspots)
- A prioritized “quick wins” list with risk/effort notes so engineering could execute safely
- A lightweight monthly optimization plan to prevent cost drift

## Technical Execution (What this repo demonstrates)
This repository is a fully local, reproducible simulation of that engagement. It:
- Generates deterministic mock AWS CUR-like and GCP billing-export-like datasets
- Normalizes both into a unified canonical line-item model
- Produces a DuckDB analytics database plus CSV/JSON exports
- Generates an executive-friendly report (HTML + Markdown) and a technical dashboard (Streamlit + static snapshot)
- Produces exactly 10 prioritized quick wins with expected monthly savings and implementation notes

```mermaid
flowchart LR
  subgraph Inputs
    A[AWS CUR-like CSV] --> N[Normalize + Validate]
    G[GCP Billing Export-like CSV] --> N
    I[Inventory Snapshots CSV] --> W[Waste Detection]
    U[Utilization Signals CSV] --> W
  end

  N --> D[(DuckDB)]
  W --> D

  D --> Q[Quick Wins (Top 10)]
  D --> R[Executive Report (HTML/MD)]
  D --> S[Static Dashboard Snapshot (HTML)]
  D --> T[Streamlit Dashboard]

  Q --> O[out/*.csv + out/*.json]
  R --> O
  S --> O
```

## Quantifiable Results (from the deterministic demo data)
- Baseline spend (simulated): **~$20k/month**
- Identified savings (top 10 quick wins): **~20% (~$4k/month)** across AWS + GCP

## Lasting Value
Beyond one-off savings, the repo demonstrates the core mechanics of a sustainable FinOps loop:
tagging hygiene → visibility → opportunity detection → prioritized execution → monthly drift prevention.

---

## How to run the demo

### Fast path
```bash
make bootstrap
make demo
make report
```

### Run the dashboard
```bash
make dashboard
```

### Clean run
```bash
make clean
make bootstrap
make demo
```

## Validation / Test Commands
```bash
make verify
```

## Sample Output / Reports
After `make demo`, you will see generated artifacts in `out/`, including:
- `out/executive_report.html`
- `out/executive_report.md`
- `out/dashboard_snapshot.html`
- `out/quick_wins.csv`
- `out/tag_coverage.json`
- `out/monthly_plan.md`

Example (top 3 quick wins preview from `out/quick_wins.csv`):

| Rank | Quick win | Expected monthly savings (USD) | Confidence | Risk |
|---:|---|---:|---|---|
| 1 | Rightsize underutilized compute (prod) | 775 | high | medium |
| 2 | Rightsize underutilized compute (prod) | 600 | high | medium |
| 3 | Schedule non-prod compute off-hours | 560 | high | low |

## Why this demonstrates senior expertise
- Separates **billing facts** (line items) from **operational context** (inventory + utilization) before making recommendations.
- Produces **executive-ready artifacts** and **engineering-ready action items** with risk/effort/owner/KPI fields.
- Uses a **unified canonical model** so the same analysis patterns apply across AWS and GCP.
- Implements a realistic **FinOps hygiene loop** (allocation readiness + drift prevention), not just one-time charts.

## Trade-offs and assumptions
- All cloud calls are mocked; the goal is a **reproducible local demo** with deterministic outputs.
- Savings estimates are intentionally conservative and expressed as “expected”; real environments require validation windows and change-control.
- The datasets are synthetic but shaped like common CUR/billing export patterns; schemas are simplified to keep the demo runnable in minutes.

## Repo tour
- `cloud_cost_audit/`: Python package (CLI, models, pipeline, reporting, dashboard)
- `data/`: deterministic synthetic inputs (generated by `make demo` if missing)
- `out/`: generated outputs/artifacts (created by `make demo`)
- `iac/`: safe Terraform examples (not required for local demo execution)
- `.github/workflows/ci.yml`: CI running `make verify`
